.
â”œâ”€â”€ index.html                 # Ù…Ù„Ù Ù…ÙˆÙ‚Ø¹Ùƒ (Ø³Ù†Ø¶ÙŠÙ Ù„Ù‡ Ø¹Ù„Ø§Ù…Ø§Øª Ù‚Ø³Ù… Ø¢Ù„ÙŠ)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ scrape_rimnow.py       # ÙŠØ³Ø­Ø¨ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆÙŠØ¬Ù‡Ù‘Ø² Ø¨Ø·Ø§Ù‚Ø§Øª HTML
â”‚   â””â”€â”€ update_index.py        # ÙŠØ­Ù‚Ù† Ø§Ù„Ø¨Ø·Ø§Ù‚Ø§Øª Ø¯Ø§Ø®Ù„ index.html Ø¨ÙŠÙ† Ø¹Ù„Ø§Ù…ØªÙŠÙ†
â”œâ”€â”€ requirements.txt           # Ù…ÙƒØªØ¨Ø§Øª Ø¨Ø§ÙŠØ«ÙˆÙ†
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ update.yml         # GitHub Action Ù„Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø¬Ø¯ÙˆÙ„ Ø£Ùˆ Ø§Ù„ÙŠØ¯ÙˆÙŠ
requests==2.32.3
beautifulsoup4==4.12.3
lxml==5.2.2
python-dateutil==2.9.0.post0
<!-- START AUTO -->
<!-- Ø³ÙŠØªÙ… Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù… Ø¢Ù„ÙŠÙ‹Ø§ -->
<div class="text-slate-500">Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ Ø¢Ø®Ø± Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ø³Ø§Ø®Ø±Ø©â€¦</div>
<!-- END AUTO -->
# scripts/scrape_rimnow.py
import re, time, json, hashlib
from urllib.parse import urljoin
import requests
from bs4 import BeautifulSoup

BASE = "https://www.rimnow.net/"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; RimSatireBot/1.0; +https://fallinn2000.github.io/dailoul-news-for-git/)"
}

def normalize(text):
    t = re.sub(r"\s+", " ", text or "").strip()
    # Ø¥Ø²Ø§Ù„Ø© Ø±Ù…ÙˆØ² Ø´Ø§Ø¦Ø¹Ø©
    t = re.sub(r"[^\w\u0600-\u06FF ]+", "", t, flags=re.UNICODE)
    return t

def fetch_home():
    r = requests.get(BASE, headers=HEADERS, timeout=20)
    r.raise_for_status()
    return r.text

def parse_titles(html):
    soup = BeautifulSoup(html, "lxml")
    # Ø¬Ø±Ø¨ Ø§Ù†ØªÙ‚Ø§Ø¡ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù†ØµÙŠØ© ÙÙŠ Ø§Ù„ÙƒØªÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    links = []
    for a in soup.select("a"):
        txt = a.get_text(strip=True)
        href = a.get("href")
        if not txt or not href:
            continue
        if len(txt) < 12:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹
            continue
        if href.startswith("#"):
            continue
        links.append((normalize(txt), urljoin(BASE, href)))
    return links

def top_topics(links, k=5):
    # ØªØ¬Ù…ÙŠØ¹ Ø¨Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ù†Ø¸ÙŠÙØ©
    counts = {}
    first_url = {}
    for title, url in links:
        key = title
        counts[key] = counts.get(key, 0) + 1
        if key not in first_url:
            first_url[key] = url
    # ØªØ±ØªÙŠØ¨ Ø¨Ø§Ù„ØªÙƒØ±Ø§Ø± Ø«Ù… Ø¨Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
    ranked = sorted(counts.items(), key=lambda x: (-x[1], x[0]))[:k]
    items = []
    for title, cnt in ranked:
        items.append({"title": title, "url": first_url[title], "count": cnt})
    return items

if __name__ == "__main__":
    html = fetch_home()
    links = parse_titles(html)
    items = top_topics(links, k=5)
    print(json.dumps({"items": items}, ensure_ascii=False))
# scripts/update_index.py
import os, sys, json, re, datetime
from pathlib import Path
from subprocess import check_output

ROOT = Path(__file__).resolve().parents[1]
INDEX = ROOT / "index.html"

CARD_TMPL = """\
<article class="card rounded-3xl bg-white border border-slate-200 shadow p-6">
  <div class="flex items-start justify-between gap-4">
    <div>
      <h3 class="text-2xl font-extrabold">{title}</h3>
      <p class="text-sm text-slate-500 mt-1">Ù…ØµØ¯Ø±: Ù…ÙˆØ±ÙŠØªØ§Ù†ÙŠØ§ Ø§Ù„Ø¢Ù† â€” ØªÙƒØ±Ø§Ø± Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {count}Ã—</p>
    </div>
    <div class="flex items-center gap-2">
      <a class="inline-flex items-center gap-1 px-3 py-1 rounded-full text-sm bg-slate-100 text-slate-700 hover:bg-slate-200" href="{url}" target="_blank" rel="noopener">Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø£ØµÙ„ÙŠ</a>
    </div>
  </div>
  <p class="mt-4 leading-8">
    ØªØ¹Ù„ÙŠÙ‚ Ù…Ù‡Ø°Ù‘Ø¨: Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ÙŠØªØ±Ø¯Ù‘Ø¯ ÙƒØ£Ù†Ù‡ Ù„Ø§Ø²Ù…Ø© Ø£Ø³Ø¨ÙˆØ¹ÙŠØ©â€¦ Ù„Ø¹Ù„Ù‘ Ø§Ù„Ø­Ù„Ù‘ Ù„ÙŠØ³ ÙÙŠ ØªÙƒØ±Ø§Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø±ØŒ Ø¨Ù„ ÙÙŠ ØªÙƒØ±Ø§Ø± Ø§Ù„Ø­Ù„ÙˆÙ„ ğŸ™ƒ
  </p>
</article>"""

def run(cmd):
    return check_output(cmd, shell=True, text=True)

def render_cards():
    # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø³ÙƒØ±Ø¨Øª Ø§Ù„Ø³Ø§Ø¨Ù‚ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ 5
    data = run(f"{sys.executable} scripts/scrape_rimnow.py")
    items = json.loads(data)["items"]
    cards = []
    for it in items:
        cards.append(CARD_TMPL.format(title=it["title"], url=it["url"], count=it["count"]))
    now = datetime.datetime.utcnow().strftime("%Y-%m-%d %H:%M UTC")
    header = f'<div class="text-sm text-slate-500 mb-3">Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ« ØªÙ„Ù‚Ø§Ø¦ÙŠ: {now}</div>'
    return header + "\n" + "\n".join(cards)

def replace_between(text, start_mark, end_mark, new_html):
    pattern = re.compile(
        r"(?s)(" + re.escape(start_mark) + r")(.*?)(" + re.escape(end_mark) + r")"
    )
    if not pattern.search(text):
        raise SystemExit("Ù„Ù… Ø£Ø¬Ø¯ Ø§Ù„Ø¹Ù„Ø§Ù…ØªÙŠÙ† START/END Ø¯Ø§Ø®Ù„ index.html")
    return pattern.sub(r"\1\n" + new_html + r"\n\3", text)

if __name__ == "__main__":
    html = INDEX.read_text(encoding="utf-8")
    new_block = render_cards()
    out = replace_between(html, "<!-- START AUTO -->", "<!-- END AUTO -->", new_block)
    if out != html:
        INDEX.write_text(out, encoding="utf-8")
        print("index.html updated")
    else:
        print("no changes")
name: Auto-update site from RimNow

on:
  schedule:
    - cron: "0 6 * * *"   # 06:00 UTC = 09:00 Ù‚Ø·Ø±
  workflow_dispatch:

permissions:
  contents: write

jobs:
  update:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Scrape & update index.html
        run: |
          python scripts/update_index.py

      - name: Commit & push if changed
        run: |
          if [ -n "$(git status --porcelain)" ]; then
            git config user.name "github-actions[bot]"
            git config user.email "github-actions[bot]@users.noreply.github.com"
            git add -A
            git commit -m "Auto-update: RimNow feed"
            git push
          else
            echo "No changes"
          fi
